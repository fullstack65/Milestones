{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fullstack65/Milestones/blob/trunk/compare_softmax1_lama2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q torch numpy transformers datasets tiktoken wandb tqdm sentencepiece"
      ],
      "metadata": {
        "id": "ve_rXXrsGsYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/tcapelle/llama2.c"
      ],
      "metadata": {
        "id": "aoxMDOcGGdVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POopIjNXGJjH"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "# wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llama2.c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PF2L0aSGv1h",
        "outputId": "dcf19d0d-1fd7-44b7-b6dd-aacc38f8d6ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama2.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "run = wandb.init()\n",
        "\n",
        "# Softmax1:\n",
        "# artifact = run.use_artifact('capecape/llamac/jvvylljz_model:v29', type='model')\n",
        "\n",
        "# Baseline\n",
        "# artifact = run.use_artifact('capecape/llamac/2q4sljmu_model:v14', type='model')\n",
        "artifact = run.use_artifact('capecape/llamac/2q4sljmu_model:v0', type='model')\n",
        "\n",
        "\n",
        "# New run\n",
        "# artifact = run.use_artifact('capecape/llamac/na2dcgu6_model:v0', type='model')\n",
        "\n",
        "\n",
        "artifact_dir = artifact.download()\n",
        "\n",
        "\n",
        "ckpt = torch.load(artifact_dir+'/ckpt.pt')\n",
        "ckpt.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "xq_6Oq3nGWNN",
        "outputId": "20252714-7c56-40ee-f7a8-32e16d84d1de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjohnowhitaker\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/llama2.c/wandb/run-20230802_174235-gs100bfi</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/johnowhitaker/llama2.c/runs/gs100bfi' target=\"_blank\">crisp-salad-4</a></strong> to <a href='https://wandb.ai/johnowhitaker/llama2.c' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/johnowhitaker/llama2.c' target=\"_blank\">https://wandb.ai/johnowhitaker/llama2.c</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/johnowhitaker/llama2.c/runs/gs100bfi' target=\"_blank\">https://wandb.ai/johnowhitaker/llama2.c/runs/gs100bfi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact 2q4sljmu_model:v0, 1301.59MB. 1 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "Done. 0:1:38.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['model', 'optimizer', 'model_args', 'iter_num', 'best_val_loss', 'config'])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from model import Transformer, ModelArgs\n",
        "\n",
        "# Settings\n",
        "model_args = dict(\n",
        "    softmax=ckpt['config']['softmax'], # softmax or softmax1\n",
        "    flash=ckpt['config']['flash'],\n",
        ")\n",
        "# Additional model args\n",
        "checkpoint_model_args = ckpt[\"model_args\"]\n",
        "for k in [\"dim\", \"n_layers\", \"n_heads\", \"n_kv_heads\",\n",
        "          \"vocab_size\", \"multiple_of\", \"max_seq_len\"]:\n",
        "    model_args[k] = checkpoint_model_args[k]\n",
        "\n",
        "# Load the model\n",
        "gptconf = ModelArgs(**model_args)\n",
        "model = Transformer(gptconf)\n",
        "state_dict = ckpt[\"model\"]\n",
        "\n",
        "# fix the keys of the state dictionary - needed?\n",
        "unwanted_prefix = \"_orig_mod.\"\n",
        "for k, v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
        "model.load_state_dict(state_dict);\n",
        "model.to(\"cuda\");\n",
        "model.eval();\n",
        "print(model_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GXIF5m_GftC",
        "outputId": "0179b98b-a869-449b-e442-f8be0661ae2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "{'softmax': 'softmax', 'flash': False, 'dim': 768, 'n_layers': 12, 'n_heads': 12, 'n_kv_heads': 12, 'vocab_size': 32000, 'multiple_of': 32, 'max_seq_len': 1024}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "from tinystories import Task\n",
        "from functools import partial\n",
        "\n",
        "iter_batches = partial(\n",
        "    Task.iter_batches,\n",
        "    batch_size=ckpt['config']['batch_size'],\n",
        "    max_seq_len=ckpt['config']['max_seq_len'],\n",
        "    device='cuda',\n",
        "    num_workers=0,\n",
        ")\n",
        "\n",
        "train_batch_iter = iter_batches(\"data00\") # Need to run data prep script or upload the .bin file(s) into /content/llama2.c/data/TinyStories_all_data\n",
        "X, Y = next(train_batch_iter)\n",
        "\n",
        "X.shape, Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQ85qafZHdNd",
        "outputId": "649377b8-768e-44a2-ec0a-1a502b82b5b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created a PretokDataset with rng seed 42\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 1024]), torch.Size([32, 1024]))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hook_fn(module, input, output):\n",
        "    \"Hook function to store outputs in a .output attribute\"\n",
        "    module.output = output\n",
        "\n",
        "hook_handles = []\n",
        "for b in model.layers:\n",
        "    hook = b.attention.register_forward_hook(hook_fn)\n",
        "    hook_handles.append(hook)"
      ],
      "metadata": {
        "id": "-dva7a3lIp_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collecting some activations\n",
        "# (NB a batch might be enough to get the stats we want so this is overkill)\n",
        "acts = []\n",
        "\n",
        "for i in range(3):\n",
        "    a = []\n",
        "    with torch.no_grad():\n",
        "        X, Y = next(train_batch_iter)\n",
        "        o, _ = model(X)\n",
        "    for b in model.layers:\n",
        "        a.append(b.attention.output.cpu().unsqueeze(0))\n",
        "    acts.append(torch.cat(a, dim=0))\n",
        "\n",
        "outputs = torch.cat(acts, dim=1)\n",
        "outputs.shape\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVCsnm6jIyxg",
        "outputId": "053e456d-7cc8-4d7a-9b26-cc11ec91e7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 96, 1024, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the kurtosis of all the activations in the model\n",
        "from scipy.stats import kurtosis\n",
        "print(kurtosis(outputs.flatten()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeKUKru0I5TB",
        "outputId": "af032cb2-c897-4fb6-ad3c-459fa866c63f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.8936220105233286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean per layer\n",
        "torch.mean(torch.tensor([kurtosis(o.flatten().cpu().float()) for o in outputs]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69847AbsJBh6",
        "outputId": "e2609230-980c-4b4d-f3f4-b605f585c7dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(31.3825)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inf norm\n",
        "max([o.abs().max() for o in outputs])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYpQsJWdJI-d",
        "outputId": "39979dd0-d7f0-43d6-df5a-21d8d81cf586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(11.9557)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline at 4k (v0)\n",
        "# k(all) 3.9\n",
        "# k(avg) 31.3\n",
        "# inf norm 12\n",
        "\n",
        "\n",
        "# S1e-6 at 4k (v0)\n",
        "# k(all) 4.3\n",
        "# k(avg) 42.3\n",
        "# inf norm 8"
      ],
      "metadata": {
        "id": "5IxKSOiwVvOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ZnnH0r1VvRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OAsfDycqVvUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stats on 3 batches of tinystories (92 examples total, 1024 tokens max in each ):\n",
        "\n",
        "# Softmax1:\n",
        "# Inf norm: 24.8\n",
        "# Avg Kurtosis per layer: 95\n",
        "\n",
        "# Baseline:\n",
        "# Inf norm: 31.2\n",
        "# Avg Kurtosis per layer: 30 (14 across all activarions)"
      ],
      "metadata": {
        "id": "J7q6R5PXJcwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Softmax1:\n",
        "# [kurtosis(o.flatten().cpu().float()) for o in outputs]\n",
        "# [108.13075949093135,\n",
        "#  95.46966161052401,\n",
        "#  801.2906494227926, # <<<< just one layer that throws things off\n",
        "#  57.70108403813606,\n",
        "#  5.221310463381725,\n",
        "#  3.0735979864031826,\n",
        "#  1.2426456030982473,\n",
        "#  1.2678221315969953,\n",
        "#  2.382823072152604,\n",
        "#  3.4650911972867773,\n",
        "#  4.232136406924883,\n",
        "#  59.24261344775612]"
      ],
      "metadata": {
        "id": "KHTVmGyMMwqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Baseline:\n",
        "# [kurtosis(o.flatten().cpu().float()) for o in outputs]\n",
        "# [112.08623327119747,\n",
        "#  31.946602828938644,\n",
        "#  169.22754029100614,\n",
        "#  13.009267154142002,\n",
        "#  3.9934846124822103,\n",
        "#  2.065320089852139,\n",
        "#  1.377368844087628,\n",
        "#  1.2992505599665511,\n",
        "#  1.7043503114501695,\n",
        "#  2.541497231649754,\n",
        "#  2.893608796813403,\n",
        "#  11.769736114437878]"
      ],
      "metadata": {
        "id": "X_x33-WLMlSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "obQ0DVHcMmcw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}